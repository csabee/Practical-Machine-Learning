---
title: 'Practical Machine Learning: Course Project'
author: "Csaba Sarkadi"
date: '2016 április 16 '
output: html_document
---

# Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

# Data preparation

First we will download the files to R studio's working directory, than load them to data tables.

## Load libraries

```{r libraries}
library(caret)
library(randomForest)
library(e1071)
library(lattice)
library(ggplot2)
library(parallel)
library(doParallel)

```

## Download files

```{r data_setup}
setInternet2(TRUE)

target <- "pml_training.csv"
if (!file.exists(target)) {
    url <-
        "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    target <- "pml_training.csv"
    download.file(url, destfile = target)
}
training <- read.csv(target, na.strings = c("NA","#DIV/0!",""))

target <- "pml_testing.csv"
if (!file.exists(target)) {
    url <-
        "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url, destfile = target)
}
testing <- read.csv(target, na.strings = c("NA","#DIV/0!",""))

```

## Cleanup predictors

WE should reduce the number of predictors by removing columns that are useless (eg: has near zero values, NA, or is empty value).

```{r cleanup}
# Remove columns with Near Zero Values
subTrain <- training[, names(training)[!(nzv(training, saveMetrics = T)[, 4])]]

# Remove columns with NA or is empty
subTrain <- subTrain[, names(subTrain)[sapply(subTrain, function (x) ! (any(is.na(x) | x == "")))]]

# Remove V1 which seems to be a serial number, and
# cvtd_timestamp that is unlikely to influence the prediction
subTrain <- subTrain[,-1]
subTrain <- subTrain[, c(1:3, 5:58)]
```


# Separate data for Cross Validation

Using the training data, we need to separate a set to be used for the validation process.

```{r divide_data}
# Divide the training data into a training set and a validation set
inTrain <- createDataPartition(subTrain$classe, p = 0.6, list = FALSE)
subTraining <- subTrain[inTrain,]
subValidation <- subTrain[-inTrain,]
```


# Prediction model - random forest

Using the first set of data, we create the prediction model with random forest. 

```{r random_forest}
# Check if model file exists
model <- "modelFit.RData"
if (!file.exists(model)) {

    # If not, set up the parallel clusters.  
    cl <- makeCluster(detectCores() - 1)
    registerDoParallel(cl)
    
    fit <- train(subTraining$classe ~ ., method = "rf", data = subTraining)
    save(fit, file = "modelFit.RData")
  
    stopCluster(cl)
} else {
    # Good model exists from previous run, load it and use it.  
    load(file = "modelFit.RData", verbose = TRUE)
}
```

# Accuracy and Sample Error of the prediction model

We use the training subset and create a prediction. Then, we need to measure it’s accuracy.

```{r prediction}
predTrain <- predict(fit, subTraining)
```

## Accuracy measurements

```{r accuracy}
confusionMatrix(predTrain, subTraining$classe)
```

We are using the validation subset to create a prediction, then measure it’s accuracy. 
From the training subset, the accuracy is very high, at above 99%. 
Sample error is 0.0008.

```{r validation}
predValidation <- predict(fit, subValidation)
confusionMatrix(predValidation, subValidation$classe)
```

Looking at the validation subset, the accuracy is still very high, at above 99%, with an out-of-sample error of 0.0003. 
Not significantly different from the sample error.

Given the level of accuracy, we don't need to build another prediction model for better results, or to stack multiple prediction models.

From the model, we need to find the list of important predictors.

```{r predictors}
varImp(fit)
```

And the finalization.

```{r final}
fit$finalModel
```

The reported OOB Estimated Error is 12%. 
Based on the validation accuracy at over 99% and Cross-Validation out-of-sample error rate of 0.03%, with CI between 99.87% to 99.97%, the prediction model can be applied to the final testing set, and predict the classes in the 20 test cases.

# Apply prediction model

Apply the prediction model

We apply the prediction model to the testing data. 
The predicted classification are (and were 100% accurate):

```{r apply_prediction}
predTesting <- predict(fit, testing)
predTesting
```


Generate the files for submission.

```{r generate_files}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(predTesting)
```


# Conclusion

The model predicted the 20 test cases with 100% accuracy. So we are happy.